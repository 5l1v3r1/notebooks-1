{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAPIDS cuGraph and cuSpatial to analyze airport and flight data\n",
    "## Intro\n",
    "We have airports and flights datasets.  We have cuGraph and cuSpatial.  What craziness can we get up to here?\n",
    "\n",
    "We're going to use cuGraph and cuSpatial to answer these questions of our data:\n",
    "1. Which airport is the most trafficked airport in our dataset?\n",
    "1. What are the max number of plane rides (hops) do you need to take to get from the most trafficked airport to get to any other airport in our dataset?\n",
    "1. How many hops do you need to take to get from the most trafficked airport to one of the least trafficked airport?\n",
    "1. How far is that distance really?\n",
    "1. What is the topology of our airport network, based on our dataset and distance from one another?\n",
    "\n",
    "Note: The Airports data in this toy dataset is using hashed identifiers. In the beginning, this may throw you for a loop, but by the end of the notebook everything will be clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Gathering/Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cuspatial, cugraph, cudf, cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/rapidsai/cuDataShader/master/cudatashader-notebooks/data/airports.csv\n",
    "!wget https://raw.githubusercontent.com/rapidsai/cuDataShader/master/cudatashader-notebooks/data/flights.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "fdf = cudf.read_csv(data_dir+'flights.csv')\n",
    "adf = cudf.read_csv(data_dir+'airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep\n",
    "Since we'll be using cuGraph, which uses int32, and the above dtypes are int64, we recast each Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['ORIGIN_AIRPORT_ID'] = fdf['ORIGIN_AIRPORT_ID'].astype(np.int32)\n",
    "fdf['DEST_AIRPORT_ID'] = fdf['DEST_AIRPORT_ID'].astype(np.int32)\n",
    "fdf['PASSENGERS'] = fdf['PASSENGERS'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, better!  Now let's make some some graphs.  Why?  Cause graphs are fun and informative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "Recall that we're going to ask these questions of our data:\n",
    "1. Which airport is the most trafficked airport in our dataset?\n",
    "1. What are the max number of plane rides (hops) do you need to take to get from the most trafficked airport to get to any other airport in our dataset?\n",
    "1. How many hops do you need to take to get from the most trafficked airport to one of the least trafficked airport?\n",
    "1. How far is that distance really?\n",
    "1. What is the topology of our airport network, based on our dataset and distance from one another?\n",
    "\n",
    "**Let's get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cugraph.Graph()\n",
    "G.add_edge_list(fdf[\"ORIGIN_AIRPORT_ID\"], fdf[\"DEST_AIRPORT_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cudf uses the same formatting controls as Pandas!\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "fdf[\"ORIGIN_AIRPORT_ID\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf[\"DEST_AIRPORT_ID\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Which airport is the most trafficked airport in our dataset?\n",
    "\n",
    "The easiest way to find out which airport is the most trafficked is the same way Google does it for websites: Pagerank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page = cugraph.pagerank(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a graph, `df_page`.  Great!  What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pagerank isn't ordered by rank, but by vertex number, but it is easy to find the max rank and sort the orders.  Let's get our max and the top 10 airports in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_max = df_page['pagerank'].max()\n",
    "print(pr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_pr = df_page.sort_values('pagerank', ascending=False)\n",
    "sort_pr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_pr = df_page.sort_values('pagerank', ascending=False) # Just for fun, we're looking to see which airports have the least traffic\n",
    "sort_pr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the top 10 trafficked airports.  While it was easy to see from the origin and destination airports counts that 13930 would be the most trafficked, the order of the others in the list required a bit more work.  It is also interesting that no single airport acconts for 1% of the total flights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: max number of plane rides (hops)?\n",
    "\n",
    "Let's do a breadth first search (BFS) on the airports to fly out of and see how many hops it takes to get from popular airport, 13930, to an isolated one.  We'll do the BFS from the most poular airport to a randomly chosen one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cugraph.bfs(G,13930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predecessor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm...what's `-1`?  Why does it's value so high?  Well, maybe it doesn't matter...let's get the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"distance\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whoa!**  That distance value is unexpected...but really not.  In the BFS demo, Brad told us that this occurs because the isolated vertex, 0, is unreachable.  Whenever a graph contains disjointed components, the distance to the unconnected vertices will always be max_int.  He also showed us how to fix it by dropping all insanely large distances.  We'll keep `df` untouched, in case we need it again, and make a second dataframe `df2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all large distances \n",
    "exp=\"distance < 100\"\n",
    "df2 = df.query(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['predecessor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better!  A positive number has the most, and it's of course, airport 13930.  Now, let's see what the real graph distance is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"distance\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great!  We know that no matter what, in the US, you're no more than 5 flights away from any other airport.  \n",
    "\n",
    "### Question 3: How many hops do you need to take to get from the most trafficked airport to one of the least trafficed airport\n",
    "Let's find out how many flights it takes to get us to a remote airport.  Let's pick one that has 1 flight from it.  I'm choosing `16838`, but you can change that value to another airport.  Also, there's a helper function to help make it a nicer print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_airport = 16838 # change to any other airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_path(df, id):\n",
    "    \n",
    "    # Use the BFS predecessors and distance to trace the path \n",
    "    # from vertex id back to the starting vertex ( vertex 1 in this example)\n",
    "    dist = df['distance'][id]\n",
    "    lastVert = id\n",
    "    for i in range(dist):\n",
    "        nextVert = df['predecessor'][lastVert]\n",
    "        d = df['distance'][lastVert]\n",
    "        print(\"Airport \" + str(lastVert) + \" was reached from airport \" + str(nextVert) + \n",
    "        \" where the graph distance to Airport 13930 was \" + str(d) )\n",
    "        lastVert = nextVert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_path(df, end_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used my number, it would take 3 flights So now we know which airports you would connect to between those two airports.  But that is the graph distance.  What about the real distances?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:  How far is that distance really?\n",
    "Well, for that, we need to bring in our other dataset, `adf`, which is a list of the airport's latitude and longitudes, as well as the GPU accelerated `cuSpatial` library to compute the Haversine distances (distances on the surface of the globe [sphere] instead of a straight line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a new function that calculates the haversine distance of all the airports in our flights at once.  This is a great time to use merge().  We'll do 2 merges, first on `ORIGIN_AIRPORT_ID` and then on `DEST_AIRPORT_ID`. To do the merge, we'll need to typecast the queries on our original 2 dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['AIRPORT_ID'] = fdf['ORIGIN_AIRPORT_ID'].astype(np.int64) # create a common key with origin airport\n",
    "hdf = fdf.merge(adf, on=['AIRPORT_ID'], how='left')\n",
    "hdf.rename(columns = {'LATITUDE': 'LATITUDE_O', 'LONGITUDE': 'LONGITUDE_O'}, inplace=True) # Origin lat and long\n",
    "hdf['AIRPORT_ID'] = hdf['DEST_AIRPORT_ID'].astype(np.int64) # recreate a common key with destination airport\n",
    "hdf = hdf.merge(adf, on=['AIRPORT_ID'], how='left')\n",
    "hdf.rename(columns = {'LATITUDE': 'LATITUDE_D', 'LONGITUDE': 'LONGITUDE_D'}, inplace=True) # Origin lat and long\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = hdf[\"LONGITUDE_O\"]\n",
    "y1 = hdf[\"LATITUDE_O\"]\n",
    "x2 = hdf[\"LONGITUDE_D\"]\n",
    "y2 = hdf[\"LATITUDE_D\"]\n",
    "\n",
    "hdf['H-distance'] = cuspatial.haversine_distance(x1, y1, x2, y2)\n",
    "hdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the actual distances that one must fly to get between those airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = cugraph.Graph()\n",
    "#hdf[\"ORIGIN_AIRPORT_ID_0\"] = hdf[\"ORIGIN_AIRPORT_ID\"] - 10001\n",
    "#hdf[\"DEST_AIRPORT_ID_0\"] = hdf[\"DEST_AIRPORT_ID\"] - 10001\n",
    "#hdf[\"data\"] = 1.0\n",
    "H.add_edge_list(hdf[\"ORIGIN_AIRPORT_ID\"], hdf[\"DEST_AIRPORT_ID\"], hdf[\"H-distance\"])\n",
    "hgdf = cugraph.bfs(H,13930)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun Fact** Deleting the -1s throws off your indexes and doesn't return you a valid answer.  Try it if you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dist_path(df, id):\n",
    "    # Use the BFS predecessors and distance to trace the path \n",
    "    # from vertex id back to the starting vertex ( vertex 1 in this example)\n",
    "    dist = df['distance'][id]\n",
    "    hdist = 0\n",
    "    print(\"Your overall flight has \" + str(dist) + \" hops\")\n",
    "    lastVert = id\n",
    "    for i in range(dist):\n",
    "        nextVert = df['predecessor'][lastVert]\n",
    "        d = df['distance'][lastVert]\n",
    "        a = hdf.query(\"ORIGIN_AIRPORT_ID == @nextVert and DEST_AIRPORT_ID == @lastVert\")\n",
    "        a.head()\n",
    "        hdist = hdist+ a[\"H-distance\"][0]\n",
    "        print(\"Airport: \" + str(lastVert) + \" was reached from Airport \" + str(nextVert) + \n",
    "        \" and flight distance was \" + str(a[\"H-distance\"][0]) )\n",
    "        lastVert = nextVert\n",
    "    print(\"Your total flying distance was \" + str(hdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dist_path(hgdf, 16838)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, pretty cool.  We now know the distance between these airports...but where are they in the world?  Normally, we'd use use [cuDataShader](https://github.com/rapidsai/cuDataShader) for this, but it is not a library in this container.  [They've got a great example here that you can adapt to your needs](https://github.com/rapidsai/cuDataShader/blob/master/cudatashader-notebooks/cuDatashader%20Edge%20Bundling%20(US%20air%20traffic).ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: What is the topology of our airport network\n",
    "\n",
    "Let's look at the topology of this network of airports.  One way to do that is to measure the modularity of our airport system!  To do that, we use Louvain.  However, we need to make some changes to our data, as Louvain requires us to start from 0.  It also requires weights.  Let's see how weights change our answer.  We will use our Haversine distances as our weights in one set, and be unweighted in the next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = cugraph.Graph()\n",
    "L2 = cugraph.Graph()\n",
    "hdf[\"ORIGIN_AIRPORT_ID_0\"] = hdf[\"ORIGIN_AIRPORT_ID\"] - 10001\n",
    "hdf[\"DEST_AIRPORT_ID_0\"] = hdf[\"DEST_AIRPORT_ID\"] - 10001\n",
    "hdf[\"data\"]= 1.0\n",
    "L2.add_edge_list(hdf[\"ORIGIN_AIRPORT_ID_0\"], hdf[\"DEST_AIRPORT_ID_0\"], hdf[\"data\"]) # Unweighted Modularity\n",
    "L.add_edge_list(hdf[\"ORIGIN_AIRPORT_ID_0\"], hdf[\"DEST_AIRPORT_ID_0\"], hdf[\"H-distance\"]) # Distance Weighted Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Louvain on the graph\n",
    "hgdf, mod = cugraph.louvain(L) \n",
    "hgdf2, mod2 =cugraph.louvain(L2) \n",
    "# Print the modularity score\n",
    "print('Modularity using Distance as a weight was {}'.format(mod))\n",
    "print()\n",
    "print('Modularity unweighted was {}'.format(mod2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdf2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a high partition number for both graphs.  This is of course, based on a small dataset of flights.  I'll be working on a larger one in notebooks_contrib that uses DOT 2015 Flight data and use cuDataShader for graph visualizations.  Let's see what the value counts look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hgdf['partition'].unique()))\n",
    "print(len(hgdf2['partition'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdf['partition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdf2['partition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the unweighted graph is less modular.  Let's remove paritions of 1 from the ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mod(df):\n",
    "    val_counts = df['partition'].value_counts()\n",
    "    relevant_partitions = val_counts[val_counts>1].index\n",
    "    print(len(relevant_partitions))\n",
    "    query = 'partition == '+ str(relevant_partitions[0])\n",
    "    for i in range (1, len(relevant_partitions)):\n",
    "            query += ' or partition == '+ str(relevant_partitions[i])\n",
    "    return df.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many partitions where found\n",
    "def get_partitions(df):\n",
    "    part_ids = df[\"partition\"].unique()\n",
    "    for p in range(len(part_ids)):\n",
    "        part = []\n",
    "        for i in range(len(df)):\n",
    "            #print(df['partition'][i])\n",
    "            if (df['partition'][i] == part_ids[p]):\n",
    "                part.append(df['vertex'][i] +1+10001)\n",
    "        print(\"Partition \" + str(part_ids[p]) + \" contains these airports:\")\n",
    "        print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of partitions > 1 in Distance Weighted Modularity:\")\n",
    "hgdf_1 = get_mod(hgdf)\n",
    "print(\"Number of partitions > 1 in Unweighted Modularity:\")\n",
    "hgdf_2 = get_mod(hgdf2)\n",
    "\n",
    "hgdf_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------Distance Weighted Modularity------\")\n",
    "get_partitions(hgdf_1)\n",
    "print(\"------Unweighted Modularity------\")\n",
    "get_partitions(hgdf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great!  Now we know what each partition is, you can once again use [cuDataShader](https://github.com/rapidsai/cuDataShader) or [cuXFilter](https://github.com/rapidsai/cuxfilter) to visualize the results.  Let's make a pretty picture (that sound you just heard was Allan Enemark grinding his teeth :).  He's a friend, so I shouldn't befall any physcal harm by his hands.  He also leads the team that does data visualizations, and their libraries, such as [cuXFilter](https://github.com/rapidsai/cuxfilter) and [cuDataShader](https://github.com/rapidsai/cuDataShader))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
