{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Inference Library\n",
    "The forest inference library is used to load saved forest models of xgboost, lightgbm or protobuf and perform inference on them. It can be used to perform both classification and regression. This notebook shows how to use the Forest Inference library with xgboost and lightgbm models.\n",
    "\n",
    "The model accepts both numpy arrays and cuDF dataframes. In order to convert your dataset to cudf format please read the cudf documentation on https://rapidsai.github.io/projects/cudf/en/latest/. \n",
    "\n",
    "For additional information on the forest inference library please refer to the documentation on https://rapidsai.github.io/projects/cuml/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/saljain/miniconda3/envs/fil-test/lib/python3.7/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n",
      "/home/nfs/saljain/miniconda3/envs/fil-test/lib/python3.7/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "from cuml import ForestInference\n",
    "from cuml.test.utils import array_equal\n",
    "from cuml.utils.import_utils import has_xgboost, has_lightgbm\n",
    "\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_xgboost():\n",
    "    import xgboost as xgb\n",
    "else:\n",
    "    raise(\"Please install xgboost using the conda package,\"\n",
    "          \" Use conda install -c conda-forge xgboost \"\n",
    "          \"command to install xgboost\")\n",
    "    \n",
    "if has_lightgbm():\n",
    "    import lightgbm as lgb\n",
    "else:\n",
    "    raise(\"Please install lightgbm using the conda package,\"\n",
    "          \" Use conda install -c conda-forge lightgbm \"\n",
    "          \"command to install lightgbm\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification and regression data\n",
    "def simulate_data(m, n, k=2, random_state=None, classification=True):\n",
    "    if classification:\n",
    "        features, labels = make_classification(n_samples=m,\n",
    "                                               n_features=n,\n",
    "                                               n_informative=int(n/5),\n",
    "                                               n_classes=k,\n",
    "                                               random_state=random_state)\n",
    "    else:\n",
    "        features, labels = make_regression(n_samples=m,\n",
    "                                           n_features=n,\n",
    "                                           n_informative=int(n/5),\n",
    "                                           n_targets=1,\n",
    "                                           random_state=random_state)\n",
    "    return np.c_[features].astype(np.float32), \\\n",
    "        np.c_[labels].astype(np.float32).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for additional information on the xgboost library please refer to the documentation on : \n",
    "#### https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that trains the xgboost model and performs prediction on it as well\n",
    "def train_xgboost_model(X_train, y_train,\n",
    "                        X_validation,\n",
    "                        y_validation,\n",
    "                        num_rounds, classification):\n",
    "\n",
    "    # set the xgboost model parameters\n",
    "    xgboost_params={}\n",
    "    params = {'silent': 1}\n",
    "    if classification:\n",
    "        params['eval_metric'] = 'error'\n",
    "        params['objective'] = 'binary:logistic'\n",
    "    else:\n",
    "        params['eval_metric'] = 'error'\n",
    "        params['objective'] = 'reg:squarederror'\n",
    "        params['base_score'] = 0.0\n",
    "    params['max_depth'] = 25\n",
    "    params.update(xgboost_params)\n",
    "    model_path = \"xgb.model\"\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "    # save the trained xgboost model\n",
    "    bst.save_model(model_path)\n",
    "\n",
    "    # predict the xgboost model\n",
    "    dvalidation = xgb.DMatrix(X_validation, label=y_validation)\n",
    "    xgb_preds = bst.predict(dvalidation)\n",
    "\n",
    "    # if the model is used for classification then convert\n",
    "    # the predicted values into class labels\n",
    "    if classification:\n",
    "        xgb_preds = np.around(xgb_preds)\n",
    "\n",
    "    return xgb_preds, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for additional information on the lightgbm library please refer to the documentation on : \n",
    "#### https://lightgbm.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that trains the lightgbm model and performs prediction on it as well\n",
    "def train_lightgbm_model(X_train, y_train,\n",
    "                        X_validation,\n",
    "                        num_round):\n",
    "    # convert the data into the lightgbm input format\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    # select the params for the lightgbm model\n",
    "    param = {'objective': 'binary',\n",
    "             'metric': 'binary_logloss'}\n",
    "\n",
    "    # train the lightgbm model\n",
    "    bst = lgb.train(param, train_data, num_round)\n",
    "    # perform prediction on the lightgbm model\n",
    "    gbm_preds = bst.predict(X_validation)\n",
    "\n",
    "    # path where the model is saved\n",
    "    model_path = \"lgb.model\"\n",
    "    bst.save_model(model_path)\n",
    "\n",
    "    return gbm_preds, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for creating the dataset\n",
    "classification = False\n",
    "n_rows = 10000\n",
    "n_columns = 100\n",
    "n_categories = 2\n",
    "random_state = np.random.RandomState(43210)\n",
    "\n",
    "# select the model on which you want to perform\n",
    "# inference\n",
    "select_model = 'xgboost'\n",
    "\n",
    "# num of iterations for which the model is trained\n",
    "num_rounds = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "X, y = simulate_data(n_rows, n_columns, n_categories,\n",
    "                     random_state=random_state,\n",
    "                     classification=classification)\n",
    "n_rows, n_columns = X.shape\n",
    "train_size = 0.8\n",
    "\n",
    "#split the dataset into training and validation splits\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, train_size=train_size)\n",
    "\n",
    "if select_model == 'xgboost':\n",
    "    trained_model_preds, model_path = train_xgboost_model(X_train, y_train,\n",
    "                                                          X_validation,\n",
    "                                                          y_validation,\n",
    "                                                          num_rounds,\n",
    "                                                          classification)\n",
    "elif select_model == 'lightgbm':\n",
    "    trained_model_preds, model_path = train_lightgbm_model(X_train,\n",
    "                                                           y_train,\n",
    "                                                           X_validation,\n",
    "                                                           num_rounds)\n",
    "else:\n",
    "    raise(\" This model is not supported, please choose either\"\n",
    "          \" an xgboost model or lightgbm model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The load function of the ForestInference class accepts the following parameters:\n",
    "        filename : str\n",
    "           Path to saved model file in a treelite-compatible format\n",
    "           (See https://treelite.readthedocs.io/en/latest/treelite-api.html\n",
    "        output_class : bool\n",
    "           If true, return a 1 or 0 depending on whether the raw prediction\n",
    "           exceeds the threshold. If False, just return the raw prediction.\n",
    "        threshold : float\n",
    "           Cutoff value above which a prediction is set to 1.0\n",
    "           Only used if the model is classification and output_class is True\n",
    "        algo : string\n",
    "           Which inference algorithm to use.\n",
    "           See documentation in FIL.load_from_treelite_model\n",
    "        model_type : str\n",
    "            Format of saved treelite model to load.\n",
    "            Can be 'xgboost', 'lightgbm', or 'protobuf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved xgboost model to return the forest in the format used as an input by the forest inference library\n",
    "fm = ForestInference.load(filename=model_path,\n",
    "                          algo='BATCH_TREE_REORG',\n",
    "                          output_class=classification,\n",
    "                          threshold=0.50,\n",
    "                          model_type=select_model)\n",
    "# perform prediction on the model loaded from path\n",
    "fil_preds = fm.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the labels predicted with the selected models and the \n",
    "# labels predicted by the ForestInference library are similar or not\n",
    "array_equal(trained_model_preds, fil_preds, tol=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
